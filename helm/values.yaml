externalSecrets:
  enabled: false
  name: common-external-secrets
  refreshInterval: "24h"
  secretStoreRef:
    kind: ClusterSecretStore
    name: gcp-secret-manager
  target:
    name: common-secrets
    creationPolicy: Owner
  data:
    secretRef:
      # gcp service account
      - secretKey: gcp-service-account
        remoteRefKey: dev-gcp-service-account

      # hive
      - secretKey: HIVE_SERVICE_OPTS
        remoteRefKey: dev-hive-thrift-secret

      # superset postgres
      - secretKey: SUPERSET_DB_USER
        remoteRefKey: dev-superset-db-user
      - secretKey: SUPERSET_DB_PASSWORD
        remoteRefKey: dev-superset-db-password
      - secretKey: SUPERSET_DB_HOST
        remoteRefKey: dev-superset-db-host
      - secretKey: SUPERSET_DB_PORT
        remoteRefKey: dev-superset-db-port
      - secretKey: SUPERSET_DB_NAME
        remoteRefKey: dev-superset-db-name

      # superset redis
      - secretKey: SUPERSET_REDIS_PROTO
        remoteRefKey: dev-superset-redis-proto
      - secretKey: SUPERSET_REDIS_PASSWORD
        remoteRefKey: dev-superset-redis-password
      - secretKey: SUPERSET_REDIS_HOST
        remoteRefKey: dev-superset-redis-host
      - secretKey: SUPERSET_REDIS_PORT
        remoteRefKey: dev-superset-redis-port
      - secretKey: SUPERSET_REDIS_SSL_CERT_REQS
        remoteRefKey: dev-superset-redis-ssl-cert-reqs

      # superset secret key
      - secretKey: SUPERSET_SECRET_KEY
        remoteRefKey: dev-superset-secret-key

      # superset admin
      - secretKey: SUPERSET_ADMIN_USER_PASSWORD
        remoteRefKey: dev-superset-admin-password
      - secretKey: SUPERSET_ADMIN_USER_USERNAME
        remoteRefKey: dev-superset-admin-username
      - secretKey: SUPERSET_ADMIN_USER_EMAIL
        remoteRefKey: dev-superset-admin-email

tls:
  enabled: false
  secretStoreRef:
    kind: ClusterSecretStore
    name: gcp-secret-manager
  data:
    - name: trino-tls-secret
      crt: dev-trino-tls-certificate
      key: dev-trino-tls-key
    - name: superset-tls-secret
      crt: superset-tls-certificate
      key: superset-tls-key
    - name: hive2-tls-secret
      crt: hive2-tls-certificate
      key: hive2-tls-key

hive:
  enabled: false
  replicaCount: 1
  image:
    repository: ghcr.io/nil1729/trino-hive-demo
    pullPolicy: IfNotPresent
    tag: hive-master

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: "hive"

  serviceAccount:
    create: false
    automount: false
    annotations: {}
    name: ""

  configMap:
    name: hive-config-map
    core-site-xml: |
      <configuration>
        <property>
            <name>google.cloud.auth.type</name>
            <value>SERVICE_ACCOUNT_JSON_KEYFILE</value>
        </property>
        <property>
            <name>google.cloud.auth.service.account.json.keyfile</name>
            <value>/opt/secrets/gcs-sa.json</value>
        </property>
      </configuration>

  volumes:
    - name: secret-vol
      secret:
        secretName: common-secrets
        optional: false
        items:
          - key: gcp-service-account
            path: gcs-sa.json
    - name: config-map-vol
      configMap:
        name: hive-config-map
        items:
          - key: core-site-xml
            path: core-site.xml

  volumeMounts:
    - name: secret-vol
      mountPath: "/opt/secrets/gcs-sa.json"
      subPath: gcs-sa.json
      readOnly: true
    - name: config-map-vol
      mountPath: "/opt/hadoop/etc/hadoop/core-site.xml"
      subPath: core-site.xml
      readOnly: false

  env:
    - name: HIVE_AUX_JARS_PATH
      value: "/opt/hadoop/share/hadoop/tools/lib/*"
    - name: METASTORE_AUX_JARS_PATH
      value: "/opt/hadoop/share/hadoop/tools/lib/*"
    - name: SERVICE_NAME
      value: "metastore"
    - name: VERBOSE
      value: "true"
    - name: DB_DRIVER
      value: "postgres"
    - name: SERVICE_OPTS
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: HIVE_SERVICE_OPTS

  service:
    type: ClusterIP
    name: thrift
    port: 9083
    targetPort: 9083
    protocol: TCP

  livenessProbe:
    tcpSocket:
      port: 9083
    initialDelaySeconds: 30
    periodSeconds: 60
    timeoutSeconds: 10
    failureThreshold: 5

  readinessProbe:
    tcpSocket:
      port: 9083
    initialDelaySeconds: 30
    periodSeconds: 60
    timeoutSeconds: 10
    failureThreshold: 5

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80

  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  podLabels: {}
  podSecurityContext: {}
  securityContext: {}

  hiveServer2:
    enabled: true
    livenessProbe:
      tcpSocket:
        port: 10000
      initialDelaySeconds: 30
      periodSeconds: 60
      timeoutSeconds: 10
      failureThreshold: 5
    readinessProbe:
      tcpSocket:
        port: 10000
      initialDelaySeconds: 30
      periodSeconds: 60
      timeoutSeconds: 10
      failureThreshold: 5
    service:
      type: ClusterIP
      ports:
        - name: hive2
          port: 10000
          targetPort: 10000
          protocol: TCP
        - name: http
          port: 10002
          targetPort: 10002
          protocol: TCP
    ingress:
      enabled: true
      className: "contour"
      annotations:
        ingress.kubernetes.io/force-ssl-redirect: "true"
      targetPort: 10002
      hosts:
        - host: hive2.pwa-demo-nil1729.gke.dev.nilanjandeb.com
          paths:
            - path: /
              pathType: ImplementationSpecific
      tls:
        - secretName: hive2-tls-secret
          hosts:
            - hive2.pwa-demo-nil1729.gke.dev.nilanjandeb.com

trino:
  enabled: false
  nameOverride: trino
  coordinatorNameOverride: trino-coordinator
  workerNameOverride: trino-worker

  image:
    registry: ""
    repository: trinodb/trino
    tag: "468"
    digest: ""
    useRepositoryAsSoleImageReference: false
    pullPolicy: IfNotPresent

  imagePullSecrets: []

  server:
    workers: 1
    node:
      environment: production
      dataDir: /data/trino
      pluginDir: /usr/lib/trino/plugin
    log:
      trino:
        level: INFO
    config:
      path: /etc/trino
      http:
        port: 8080
      https:
        enabled: false
        port: 8443
        keystore:
          path: ""
      authenticationType: "PASSWORD"
      query:
        maxMemory: "4GB"
    exchangeManager:
      name: "filesystem"
      baseDir: "/tmp/trino-local-file-system-exchange-manager"
    workerExtraConfig: ""
    coordinatorExtraConfig: ""
    autoscaling:
      enabled: true
      maxReplicas: 2
      targetCPUUtilizationPercentage: 50
      targetMemoryUtilizationPercentage: 70
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 5
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
            - type: Pods
              value: 4
              periodSeconds: 15
          selectPolicy: Max

  accessControl: {}
  resourceGroups: {}
  additionalNodeProperties: []

  additionalConfigProperties:
    - internal-communication.shared-secret=random-value-999
    - http-server.process-forwarded=true

  additionalLogProperties: []
  additionalExchangeManagerProperties: []
  eventListenerProperties: []

  additionalCatalogs:
    iceberg: |
      connector.name=iceberg
      iceberg.catalog.type=hive_metastore
      hive.metastore.uri=thrift://hive:9083
      iceberg.file-format=PARQUET
      fs.native-gcs.enabled=true
      gcs.project-id=pwa-demo-nil1729
      gcs.json-key-file-path=/secrets/gcp-service-account
      hive.metastore.thrift.client.read-timeout=600s
      hive.metastore.thrift.client.connect-timeout=120s
    hive: |
      connector.name=hive
      hive.metastore.uri=thrift://hive:9083
      hive.non-managed-table-writes-enabled=true
      fs.native-gcs.enabled=true
      gcs.project-id=pwa-demo-nil1729
      gcs.json-key-file-path=/secrets/gcp-service-account
      hive.metastore.thrift.client.read-timeout=600s
      hive.metastore.thrift.client.connect-timeout=120s

  env: []
  envFrom: []
  initContainers: {}
  sidecarContainers: {}

  securityContext:
    runAsUser: 1000
    runAsGroup: 1000

  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL

  shareProcessNamespace:
    coordinator: false
    worker: false

  service:
    annotations: {}
    type: ClusterIP
    port: 8080
    nodePort: ""

  auth:
    passwordAuth: |
      admin:$2y$10$TaIpQC.dnjGzoxiYYMuS/uQqSBEYJ4H3n4s4Rq8da7bVCBrdox.fW

  serviceAccount:
    create: false
    name: ""
    annotations: {}

  configMounts: []
  secretMounts:
    - name: common-secrets
      secretName: common-secrets
      path: /secrets

  coordinator:
    jvm:
      maxHeapSize: "8G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"

    config:
      memory:
        heapHeadroomPerNode: ""
      query:
        maxMemoryPerNode: "1GB"

    additionalJVMConfig: []
    additionalExposedPorts: {}
    resources:
      requests:
        memory: "8G"
        cpu: "2"
      limits:
        memory: "8G"
        cpu: "2"
    livenessProbe: {}
    readinessProbe: {}
    lifecycle: {}
    terminationGracePeriodSeconds: 30
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalConfigFiles: {}
    additionalVolumes: []
    additionalVolumeMounts: []
    annotations: {}
    labels: {}
    configMounts: []
    secretMounts: []

  worker:
    jvm:
      maxHeapSize: "8G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"

    config:
      memory:
        heapHeadroomPerNode: ""
      query:
        maxMemoryPerNode: "1GB"

    additionalJVMConfig: []
    additionalExposedPorts: {}
    resources:
      requests:
        memory: "8G"
        cpu: "2"
      limits:
        memory: "8G"
        cpu: "2"
    livenessProbe: {}
    readinessProbe: {}
    lifecycle: {}
    terminationGracePeriodSeconds: 30
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalConfigFiles: {}
    additionalVolumes: []
    additionalVolumeMounts: []
    annotations: {}
    labels: {}
    configMounts: []
    secretMounts: []

  kafka:
    mountPath: "/etc/trino/schemas"
    tableDescriptions: {}

  jmx:
    enabled: false
    registryPort: 9080
    serverPort: 9081
    exporter:
      enabled: false
      image: bitnami/jmx-exporter:latest
      pullPolicy: Always
      port: 5556
      configProperties: []
      securityContext: {}
      resources: {}

  serviceMonitor:
    enabled: false
    labels:
      prometheus: kube-prometheus
    interval: "30s"

  commonLabels: {}

  ingress:
    enabled: true
    className: "contour"
    annotations:
      ingress.kubernetes.io/force-ssl-redirect: "true"
    hosts:
      - host: trino.pwa-demo-nil1729.gke.dev.nilanjandeb.com
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls:
      - secretName: trino-tls-secret
        hosts:
          - trino.pwa-demo-nil1729.gke.dev.nilanjandeb.com

superset:
  enabled: false

  nameOverride: ~
  fullnameOverride: superset

  image:
    repository: ghcr.io/nil1729/trino-hive-demo
    tag: superset-master
    pullPolicy: IfNotPresent
  imagePullSecrets: []

  extraLabels: {}
  runAsUser: 0
  secretEnv:
    create: false
  serviceAccountName: ~
  serviceAccount:
    create: false
    annotations: {}

  bootstrapScript: |
    #!/bin/bash
    if [ ! -f ~/bootstrap ]; then echo "Running Superset with uid {{ .Values.runAsUser }}" > ~/bootstrap; fi

  configFromSecret: superset-config
  envFromSecret: ~

  extraEnv:
    SERVER_WORKER_CLASS: gthread
    GUNICORN_TIMEOUT: 300
    SERVER_WORKER_AMOUNT: 4
    WORKER_MAX_REQUESTS: 0
    WORKER_MAX_REQUESTS_JITTER: 0
    SERVER_THREADS_AMOUNT: 20
    GUNICORN_KEEPALIVE: 2
    SERVER_LIMIT_REQUEST_LINE: 0
    SERVER_LIMIT_REQUEST_FIELD_SIZE: 0

  extraEnvRaw: &extraEnvRaw
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_DB_USER
    - name: DB_PASS
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_DB_PASSWORD
    - name: DB_HOST
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_DB_HOST
    - name: DB_PORT
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_DB_PORT
    - name: DB_NAME
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_DB_NAME
    - name: REDIS_PROTO
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_REDIS_PROTO
    - name: REDIS_PASSWORD
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_REDIS_PASSWORD
    - name: REDIS_HOST
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_REDIS_HOST
    - name: REDIS_PORT
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_REDIS_PORT
    - name: REDIS_SSL_CERT_REQS
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_REDIS_SSL_CERT_REQS
    - name: SECRET_KEY
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_SECRET_KEY
    - name: ADMIN_USER_EMAIL
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_ADMIN_USER_EMAIL
    - name: ADMIN_USER_USERNAME
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_ADMIN_USER_USERNAME
    - name: ADMIN_USER_PASSWORD
      valueFrom:
        secretKeyRef:
          name: common-secrets
          key: SUPERSET_ADMIN_USER_PASSWORD

  envFromSecrets: []
  extraSecretEnv: {}
  extraConfigs: {}
  extraSecrets: {}
  extraVolumes: []
  extraVolumeMounts: []
  configOverrides: {}
  configOverridesFiles: {}

  configMountPath: "/app/pythonpath"
  extraConfigMountPath: "/app/configs"

  initImage:
    repository: apache/superset
    tag: dockerize
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 8088
    annotations: {}
    loadBalancerIP: ~
    nodePort:
      http: ~

  ingress:
    enabled: true
    ingressClassName: "contour"
    annotations:
      ingress.kubernetes.io/force-ssl-redirect: "true"
    path: /
    pathType: ImplementationSpecific
    hosts:
      - superset.pwa-demo-nil1729.gke.dev.nilanjandeb.com
    tls:
      []
      # - secretName: superset-tls-secret
      #   hosts:
      #     - superset.pwa-demo-nil1729.gke.dev.nilanjandeb.com
    extraHostsRaw: []

  resources: {}
  hostAliases: []

  supersetNode:
    replicas:
      enabled: true
      replicaCount: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: 1
    command:
      - "/bin/sh"
      - "-c"
      - ". {{ .Values.configMountPath }}/superset_bootstrap.sh; /usr/bin/run-server.sh"
    connections:
      redis_host: ~
      redis_port: ~
      redis_user: ~
      redis_cache_db: "1"
      redis_celery_db: "0"
      redis_password: "enabled"
      redis_ssl:
        enabled: true
        ssl_cert_reqs: ~
      db_host: ~
      db_port: ~
      db_user: ~
      db_pass: ~
      db_name: ~
    env: {}
    forceReload: false
    initContainers:
      - name: wait-for-postgres
        image: "{{ .Values.initImage.repository }}:{{ .Values.initImage.tag }}"
        imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
        env: *extraEnvRaw
        command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s

    extraContainers: []
    deploymentAnnotations: {}
    deploymentLabels: {}
    affinity: {}
    topologySpreadConstraints: []
    podAnnotations: {}
    podLabels: {}
    startupProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 15
      timeoutSeconds: 1
      failureThreshold: 60
      periodSeconds: 5
      successThreshold: 1
    livenessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 15
      timeoutSeconds: 1
      failureThreshold: 3
      periodSeconds: 15
      successThreshold: 1
    readinessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 15
      timeoutSeconds: 1
      failureThreshold: 3
      periodSeconds: 15
      successThreshold: 1
    resources: {}
    podSecurityContext: {}
    containerSecurityContext: {}
    strategy: {}

  supersetWorker:
    replicas:
      enabled: true
      replicaCount: 0
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: 1
    command:
      - "/bin/sh"
      - "-c"
      - ". {{ .Values.configMountPath }}/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app worker"
    forceReload: false
    initContainers:
      - name: wait-for-postgres-redis
        image: "{{ .Values.initImage.repository }}:{{ .Values.initImage.tag }}"
        imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
        env: *extraEnvRaw
        command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT" -timeout 120s
    extraContainers: []
    deploymentAnnotations: {}
    deploymentLabels: {}
    affinity: {}
    topologySpreadConstraints: []
    podAnnotations: {}
    podLabels: {}
    resources: {}
    podSecurityContext: {}
    containerSecurityContext: {}
    strategy: {}
    livenessProbe:
      exec:
        command:
          - sh
          - -c
          - celery -A superset.tasks.celery_app:app inspect ping -d celery@$HOSTNAME
      initialDelaySeconds: 120
      timeoutSeconds: 60
      failureThreshold: 3
      periodSeconds: 60
      successThreshold: 1
    startupProbe: {}
    readinessProbe: {}
    priorityClassName: ~

  supersetCeleryBeat:
    enabled: false
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: 1
    command:
      - "/bin/sh"
      - "-c"
      - ". {{ .Values.configMountPath }}/superset_bootstrap.sh; celery --app=superset.tasks.celery_app:app beat --pidfile /tmp/celerybeat.pid --schedule /tmp/celerybeat-schedule"
    forceReload: false
    initContainers:
      - name: wait-for-postgres-redis
        image: "{{ .Values.initImage.repository }}:{{ .Values.initImage.tag }}"
        imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
        env: *extraEnvRaw
        command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT" -timeout 120s
    extraContainers: []
    deploymentAnnotations: {}
    affinity: {}
    topologySpreadConstraints: []
    podAnnotations: {}
    podLabels: {}
    resources: {}
    podSecurityContext: {}
    containerSecurityContext: {}
    priorityClassName: ~

  supersetCeleryFlower:
    enabled: false
    replicaCount: 1
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: 1
    command:
      - "/bin/sh"
      - "-c"
      - "celery --app=superset.tasks.celery_app:app flower"
    service:
      type: ClusterIP
      annotations: {}
      loadBalancerIP: ~
      port: 5555
      nodePort:
        http: ~
    startupProbe:
      httpGet:
        path: /api/workers
        port: flower
      initialDelaySeconds: 5
      timeoutSeconds: 1
      failureThreshold: 60
      periodSeconds: 5
      successThreshold: 1
    livenessProbe:
      httpGet:
        path: /api/workers
        port: flower
      initialDelaySeconds: 5
      timeoutSeconds: 1
      failureThreshold: 3
      periodSeconds: 5
      successThreshold: 1
    readinessProbe:
      httpGet:
        path: /api/workers
        port: flower
      initialDelaySeconds: 5
      timeoutSeconds: 1
      failureThreshold: 3
      periodSeconds: 5
      successThreshold: 1
    initContainers:
      - name: wait-for-postgres-redis
        image: "{{ .Values.initImage.repository }}:{{ .Values.initImage.tag }}"
        imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
        env: *extraEnvRaw
        command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -wait "tcp://$REDIS_HOST:$REDIS_PORT" -timeout 120s
    extraContainers: []
    deploymentAnnotations: {}
    affinity: {}
    topologySpreadConstraints: []
    podAnnotations: {}
    podLabels: {}
    resources: {}
    podSecurityContext: {}
    containerSecurityContext: {}
    priorityClassName: ~

  supersetWebsockets:
    enabled: false
    replicaCount: 1
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: 1
    ingress:
      path: /ws
      pathType: Prefix
    image:
      repository: oneacrefund/superset-websocket
      tag: latest
      pullPolicy: IfNotPresent
    config:
      {
        "port": 8080,
        "logLevel": "debug",
        "logToFile": false,
        "logFilename": "app.log",
        "statsd": { "host": "127.0.0.1", "port": 8125, "globalTags": [] },
        "redis": { "port": 6379, "host": "127.0.0.1", "password": "", "db": 0, "ssl": false },
        "redisStreamPrefix": "async-events-",
        "jwtSecret": "CHANGE-ME",
        "jwtCookieName": "async-token",
      }
    service:
      type: ClusterIP
      annotations: {}
      loadBalancerIP: ~
      port: 8080
      nodePort:
        http: ~
    command: []
    resources: {}
    extraContainers: []
    deploymentAnnotations: {}
    affinity: {}
    topologySpreadConstraints: []
    podAnnotations: {}
    podLabels: {}
    strategy: {}
    podSecurityContext: {}
    containerSecurityContext: {}
    startupProbe:
      httpGet:
        path: /health
        port: ws
      initialDelaySeconds: 5
      timeoutSeconds: 1
      failureThreshold: 60
      periodSeconds: 5
      successThreshold: 1
    livenessProbe:
      httpGet:
        path: /health
        port: ws
      initialDelaySeconds: 5
      timeoutSeconds: 1
      failureThreshold: 3
      periodSeconds: 5
      successThreshold: 1
    readinessProbe:
      httpGet:
        path: /health
        port: ws
      initialDelaySeconds: 5
      timeoutSeconds: 1
      failureThreshold: 3
      periodSeconds: 5
      successThreshold: 1
    priorityClassName: ~

  init:
    resources: {}
    command:
      - "/bin/sh"
      - "-c"
      - ". {{ .Values.configMountPath }}/superset_bootstrap.sh; . {{ .Values.configMountPath }}/superset_init.sh"
    enabled: true
    jobAnnotations:
      "helm.sh/hook": post-install,post-upgrade
      "helm.sh/hook-delete-policy": "before-hook-creation"
    loadExamples: false
    createAdmin: true
    adminUser: {}
    initContainers:
      - name: wait-for-postgres
        image: "{{ .Values.initImage.repository }}:{{ .Values.initImage.tag }}"
        imagePullPolicy: "{{ .Values.initImage.pullPolicy }}"
        env: *extraEnvRaw
        command:
          - /bin/sh
          - -c
          - dockerize -wait "tcp://$DB_HOST:$DB_PORT" -timeout 120s
    # -- A Superset init script
    # @default -- a script to create admin user and initialize roles
    initscript: |-
      #!/bin/sh
      set -eu
      echo "Upgrading DB schema..."
      superset db upgrade
      echo "Initializing roles..."
      superset init
      {{ if .Values.init.createAdmin }}
      echo "Creating admin user..."
      superset fab create-admin \
                  --username "${ADMIN_USER_USERNAME}" \
                  --firstname "SUPERSET" \
                  --lastname "SUPER ADMIN" \
                  --email "${ADMIN_USER_EMAIL}" \
                  --password "${ADMIN_USER_PASSWORD}" || true
      {{- end }}
      {{ if .Values.init.loadExamples }}
      echo "Loading examples..."
      superset load_examples
      {{- end }}
      if [ -f "{{ .Values.extraConfigMountPath }}/import_datasources.yaml" ]; then
        echo "Importing database connections.... "
        superset import_datasources -p {{ .Values.extraConfigMountPath }}/import_datasources.yaml
      fi
    extraContainers: []
    podAnnotations: {}
    podLabels: {}
    podSecurityContext: {}
    containerSecurityContext: {}
    tolerations: []
    affinity: {}
    topologySpreadConstraints: []
    priorityClassName: ~

  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []
  priorityClassName: ~
